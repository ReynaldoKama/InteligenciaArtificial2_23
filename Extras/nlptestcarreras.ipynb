{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install spacy\n","!python -m spacy download es_core_news_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-mGrr2FLXEf","executionInfo":{"status":"ok","timestamp":1700678289780,"user_tz":240,"elapsed":27985,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"a86a0bae-95d3-4b16-8121-cb067eb3b7a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n","2023-11-22 18:37:47.457445: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-11-22 18:37:47.457526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-11-22 18:37:47.457572: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-11-22 18:37:48.707884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting es-core-news-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.3)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n","Installing collected packages: es-core-news-sm\n","Successfully installed es-core-news-sm-3.6.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VrYrTqv4_0VJ","executionInfo":{"status":"ok","timestamp":1700678150907,"user_tz":240,"elapsed":29530,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"d5aed42f-64cd-4df4-a3d9-368d30fb61e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import pathlib\n","# Defina una función para leer los datos de texto y devolver pares de texto y etiqueta\n","def read_text_data(data_path):\n","    texts = []\n","    labels = []\n","    label_mapping = {'sis': 0, 'dis': 1, 'cic': 2, 'tis': 3}\n","\n","    for label in label_mapping.keys():\n","        label_path = os.path.join(data_path, label)\n","        for text_file in os.listdir(label_path):\n","            with open(os.path.join(label_path, text_file), 'r', encoding='utf-8') as f:\n","                text = f.read()\n","            labels.append(label_mapping[label])\n","            texts.append(text)\n","\n","    return texts, labels"],"metadata":{"id":"YPRAiZUcCd8D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ruta de acceso al directorio del dataset\n","data_path = pathlib.Path(\"/content/drive/MyDrive/IA22023/DATASETS/NLPDatasetIng\")\n","print(data_path)\n","\n","# Leer los datos de texto y las etiquetas del directorio train\n","texts, labels = read_text_data(data_path/'train')\n","\n","print(f'Successfully read {len(texts)} texts, and {len(labels)} labels from training dataset')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFKfChS8Cm6q","executionInfo":{"status":"ok","timestamp":1700678230747,"user_tz":240,"elapsed":51762,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"9ce54af0-3a5f-4630-a992-cc810f27191a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/IA22023/DATASETS/NLPDatasetIng\n","Successfully read 3332 texts, and 3332 labels from training dataset\n"]}]},{"cell_type":"markdown","source":["#### Processing the dataset with a text tokenizer and constrcut the vocabulary\n","\n","\n","We will use the `get_tokenizer()` function in the torchtext library from\n","`torchtext.data.utils`. This function can be used to create a tokenizer\n","that will be used to preprocess the text data, it takes an argument that\n","specifies the type of tokenizer to create, we will use in this case a\n","`basic_english` tokenizer. This is a simple type of tokenizer that\n","splits the text into words based on whitespace and punctuation marks,\n","converts all words to lowercase (i.e. standardizing and tokenizing)."],"metadata":{"id":"edgRMxv5pJpI"}},{"cell_type":"code","source":["# from torchtext.data.utils import get_tokenizer\n","\n","# # Define a tokenizer function to preprocess the text\n","# tokenizer = get_tokenizer('basic_english')"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:06:52.464048Z","iopub.execute_input":"2023-05-21T07:06:52.465039Z","iopub.status.idle":"2023-05-21T07:06:56.654325Z","shell.execute_reply.started":"2023-05-21T07:06:52.464987Z","shell.execute_reply":"2023-05-21T07:06:56.653359Z"},"trusted":true,"id":"-wNnWV--pJpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spacy\n","from torchtext.data.utils import get_tokenizer\n","\n","# Cargar el modelo en español\n","nlp = spacy.load('es_core_news_sm')\n","\n","# Definir una función de tokenización que utiliza el modelo de spacy\n","def tokenize(text):\n","    return [token.text for token in nlp(text)]\n","\n","# Crear el tokenizador\n","tokenizer = get_tokenizer(tokenize)"],"metadata":{"id":"Q6miKbLpKyJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let’s see a sample standardized and tokenized text:"],"metadata":{"id":"O26SmRnWpJpI"}},{"cell_type":"code","source":["tokenizer('Este es un ejemplo de texto en español.')"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:07:32.844092Z","iopub.execute_input":"2023-05-21T07:07:32.845197Z","iopub.status.idle":"2023-05-21T07:07:32.858253Z","shell.execute_reply.started":"2023-05-21T07:07:32.845157Z","shell.execute_reply":"2023-05-21T07:07:32.857322Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"PAb4kIwSpJpJ","executionInfo":{"status":"ok","timestamp":1700678299170,"user_tz":240,"elapsed":495,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"f0a4ac94-d6b9-42b7-c5a5-9c1dd3744111"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Este', 'es', 'un', 'ejemplo', 'de', 'texto', 'en', 'español', '.']"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["Next, we’ll define a way to numercalize the tokens that can be created\n","from the previous tokenizer, in particular, we’ll index the tokens and\n","map them to the vocabulary constructed for the entire words in the text\n","corpus (i.e. indexing)."],"metadata":{"id":"9oFcfzGgpJpJ"}},{"cell_type":"code","source":["from torchtext.vocab import build_vocab_from_iterator\n","\n","# Build the vocabulary from the text data\n","vocab = build_vocab_from_iterator(map(tokenizer, texts), specials=['<unk>'])\n","vocab.set_default_index(vocab['<unk>'])\n","\n","# Define a function to numericalize the text\n","def numericalize_text(text):\n","    return [vocab[token] for token in tokenizer(text)]"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:08:25.273912Z","iopub.execute_input":"2023-05-21T07:08:25.274332Z","iopub.status.idle":"2023-05-21T07:08:30.519887Z","shell.execute_reply.started":"2023-05-21T07:08:25.274302Z","shell.execute_reply":"2023-05-21T07:08:30.518911Z"},"trusted":true,"id":"jJIPPnt0pJpJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<!-- #region id=\"UuCHfF_CaUId\" -->\n","\n","The `build_vocab_from_iterator()` allows us to build a vocabulary from\n","an iterator, which can be useful when working with large text corpus.\n","\n","`map(tokenizer, texts)` builds a vocabulary from an iterator that\n","applies the tokenizer function to each text in the texts list that we\n","created earlier. The `specials=['<unk>']` argument specifies that the\n","special token `<unk>` should be included in the vocabulary, and\n","`vocab.set_default_index(vocab['<unk>']` sets the default index of the\n","vocabulary object vocab to the index of the `<unk>` token. This means\n","that any token that is not present in the vocabulary will be replaced\n","with the `<unk>` token during numericalization.\n","\n","We’ll have some checks for the constructed vocabulary for our text set\n","and word indexes below: <!-- #endregion -->"],"metadata":{"id":"nZZ3V1HapJpJ"}},{"cell_type":"code","source":["# the length of the constructed vocab from the text set, 100683 unique tokens\n","print(len(vocab))"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:09:45.680010Z","iopub.execute_input":"2023-05-21T07:09:45.680434Z","iopub.status.idle":"2023-05-21T07:09:45.687916Z","shell.execute_reply.started":"2023-05-21T07:09:45.680402Z","shell.execute_reply":"2023-05-21T07:09:45.686809Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"-gigPyrgpJpJ","executionInfo":{"status":"ok","timestamp":1700678380990,"user_tz":240,"elapsed":503,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"567aa7db-459d-4e54-dfea-80e31cd4e06e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2749\n"]}]},{"cell_type":"code","source":["# checking the index of words that are present in the vocabulary\n","print(vocab(['este', 'es', 'un', 'ejemplo','en','español','.']))"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:09:49.458129Z","iopub.execute_input":"2023-05-21T07:09:49.458530Z","iopub.status.idle":"2023-05-21T07:09:49.467807Z","shell.execute_reply.started":"2023-05-21T07:09:49.458501Z","shell.execute_reply":"2023-05-21T07:09:49.466867Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"3uxbfN2dpJpK","executionInfo":{"status":"ok","timestamp":1700678385621,"user_tz":240,"elapsed":1125,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"edf97ffd-ac26-4711-e552-d561cde5269b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[233, 13, 21, 0, 5, 0, 3]\n"]}]},{"cell_type":"code","source":["# Al comprobar el índice de una palabra que no está presente en el vocabulario, devuelve 0, el índice de <unk>\n","print(vocab['estabilidad'])"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:09:52.201543Z","iopub.execute_input":"2023-05-21T07:09:52.201933Z","iopub.status.idle":"2023-05-21T07:09:52.208120Z","shell.execute_reply.started":"2023-05-21T07:09:52.201903Z","shell.execute_reply":"2023-05-21T07:09:52.207077Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"qGM0hEXipJpK","executionInfo":{"status":"ok","timestamp":1700678398010,"user_tz":240,"elapsed":368,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"e29644b6-ce90-4cb2-bff1-9a4950e1f6d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"markdown","source":["Now, let’s prepare the training and validation dataset that we will be\n","using in our model. Using PyTorch’s Dataset and Dataloader functionality\n","is an efficient way to manage your data and simplify your machine\n","learning workflow. By creating a Dataset, you can easily store all of\n","your data in a structured manner. Dataloader, on the other hand, allows\n","you to iterate through the data, manage batches, and perform data\n","transformations, among other tasks. Together, these tools can help make\n","your data more manageable and streamline your machine learning pipeline.\n","\n","We will build a custom text dataset from the texts and labels we created\n","earlier, to learn the basics of how to build a custom text dataset,\n","follow the tutorial in [this\n","link](https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00).\n","\n","We can numericalize the text in the train dataset in the same custom\n","dataset classs by the help of `numericalize_text()` that we defined in\n","the earlier step, we can modify the `__getitem__` method of the\n","CustomTextDataset class to apply the `numericalize_text()` function to\n","each text sample, as follows: <!-- #endregion -->"],"metadata":{"id":"eFpBG4kHpJpK"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","#  Definir una clase de conjunto de datos personalizada para los datos de texto\n","class CustomTextDataset(Dataset):\n","    def __init__(self, texts, labels, vocab, numericalize_text):\n","        self.texts = texts\n","        self.labels = labels\n","        self.vocab = vocab\n","        self.numericalize_text = numericalize_text\n","\n","    def __getitem__(self, index):\n","        label = self.labels[index]\n","        text = self.texts[index]\n","        numericalized_text = self.numericalize_text(text)\n","        return numericalized_text, label\n","\n","    def __len__(self):\n","        return len(self.labels)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:11:05.315110Z","iopub.execute_input":"2023-05-21T07:11:05.315503Z","iopub.status.idle":"2023-05-21T07:11:05.325008Z","shell.execute_reply.started":"2023-05-21T07:11:05.315474Z","shell.execute_reply":"2023-05-21T07:11:05.324088Z"},"trusted":true,"id":"BIuIFONwpJpK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the code above, we modified the `__getitem__` method to first extract\n","the text and label from the dataset, then apply the numericalize_text\n","function to the text sample to get a list of numericalized tokens. The\n","numericalized tokens are then stored in the “Text” field of the sample\n","dictionary along with the label, which is stored in the “Class” field.\n","Finally, the sample dictionary is returned.\n","\n","We also added the vocab and tokenizer arguments to the CustomTextDataset\n","constructor so that the tokenizer and vocabulary can be passed to the\n","dataset. This allows the numericalize_text function to use the\n","vocabulary and tokenizer defined earlier in the code.\n","\n","\n","Let’s create the dataset now as an object of CustomeTextDataset class,\n","and create validation set by setting apart 20% of the training dataset."],"metadata":{"id":"3x4aWujapJpL"}},{"cell_type":"code","source":["# Create train and validation datasets\n","dataset = CustomTextDataset(texts, labels, vocab, numericalize_text)\n","train_size = int(len(dataset) * 0.8)\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:12:23.566884Z","iopub.execute_input":"2023-05-21T07:12:23.567657Z","iopub.status.idle":"2023-05-21T07:12:23.595645Z","shell.execute_reply.started":"2023-05-21T07:12:23.567621Z","shell.execute_reply":"2023-05-21T07:12:23.594700Z"},"trusted":true,"id":"oaAQEgCupJpL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we’ll generate the data batch. To generate the data batch,\n","`torch.utils.data.DataLoader` is used here.\n","\n","Before sending to the model, `collate_fn` function works on a batch of\n","samples generated from `DataLoader`. The input to `collate_fn` is a\n","batch of data with the batch size in `DataLoader`, and `collate_fn`\n","processes them according to the data processing a custom pipeline that\n","we’ll define in the following function named `collate_batch`, as\n","follows."],"metadata":{"id":"iapQ1UcHpJpL"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","from torch.nn.utils.rnn import pad_sequence\n","\n","# preprocess the data with a collate function, and pads the input sequences to the maximum length in the batch:\n","def collate_batch(batch):\n","    label_list, text_list = [], []\n","    for (_text, _label) in batch:\n","        label_list.append(_label + 1)\n","        processed_text = torch.tensor(_text)\n","        text_list.append(processed_text)\n","\n","    # No es necesario verificar el rango de las etiquetas aquí\n","\n","    padded_text = pad_sequence(text_list, batch_first=False, padding_value=1.0)\n","    return torch.tensor(label_list, dtype=torch.long).to(device), padded_text.to(device)\n","\n","# Create train and validation data loaders\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, collate_fn=collate_batch, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, collate_fn=collate_batch, batch_size=batch_size, shuffle=False)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:13:39.662703Z","iopub.execute_input":"2023-05-21T07:13:39.663099Z","iopub.status.idle":"2023-05-21T07:13:39.692639Z","shell.execute_reply.started":"2023-05-21T07:13:39.663069Z","shell.execute_reply":"2023-05-21T07:13:39.691435Z"},"trusted":true,"id":"gvJMaRRjpJpL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can see the benefit of the custom defined `collate_batch()` function\n","we defined above, we used it, for instance, to pad the sequences with\n","`padding_value=1.0`, the benefit of padding the sequence with 1.0 in the\n","previous step is to make all the input sequences have the same length,\n","which is required by most deep learning models for NLP. Padding also\n","helps to preserve the information at the beginning and end of the\n","sequence, which can be important for some tasks. Padding with 1.0 is a\n","common choice because it is a neutral value that does not interfere with\n","other tokens.\n","\n","We need such a step because natural language sentences have variable\n","lengths, but deep learning models expect fixed-size inputs. For example,\n","if we want to use a recurrent neural network (RNN) to process a batch of\n","sentences, we need to pad them to the maximum length in the batch so\n","that they can be fed into the deep learning model as a matrix.\n","\n","An example of padding is shown below:\n","\n","Original sentences:\n","`[“I really love this movie”, “It was terrible”, “The acting was good”]`\n","\n","Tokenized sentences:\n","`[[9, 14, 56, 11, 17], [10, 21, 88], [8, 45, 21, 32]]`\n","\n","Padded sentences to the longest sentence in our list (with maxlen=5):\n","`[[9, 14, 56, 11, 17], [10, 21, 88 ,1 ,1], [8 ,45 ,21 ,32 ,1]]`"],"metadata":{"id":"3LQph74ZpJpL"}},{"cell_type":"markdown","source":["Let’s make a last check to see how the data is stored in our\n","`train_loader` batches:"],"metadata":{"id":"d1MK7636pJpM"}},{"cell_type":"code","source":["label, text = next(iter(train_loader))\n","print(label.shape, text.shape)\n","print(label, text)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:15:06.539583Z","iopub.execute_input":"2023-05-21T07:15:06.540028Z","iopub.status.idle":"2023-05-21T07:15:09.918345Z","shell.execute_reply.started":"2023-05-21T07:15:06.539982Z","shell.execute_reply":"2023-05-21T07:15:09.917186Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"uclqxpEbpJpM","executionInfo":{"status":"ok","timestamp":1700669923035,"user_tz":240,"elapsed":878,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"5a43ee32-273f-409c-8762-a732d4afe6bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32]) torch.Size([176, 32])\n","tensor([3, 4, 1, 1, 2, 2, 3, 4, 2, 1, 3, 3, 4, 4, 2, 2, 3, 3, 2, 1, 4, 3, 1, 4,\n","        3, 1, 3, 1, 1, 2, 1, 3]) tensor([[ 62,  18,  39,  ...,  29, 120,  39],\n","        [  7, 110, 198,  ..., 212, 319, 321],\n","        [485,  27,   9,  ...,   5,  45, 339],\n","        ...,\n","        [  1,   1,   1,  ...,   1,   1,   1],\n","        [  1,   1,   1,  ...,   1,   1,   1],\n","        [  1,   1,   1,  ...,   1,   1,   1]])\n"]}]},{"cell_type":"markdown","source":["One last note before starting the modelling, we used `batch_first=False`\n","in `pad_sequence` in our custom collate function above, and the benefit\n","is to make the padded sequences have a shape of (sequence_length,\n","batch_size), which is the expected input shape for the model in PyTorch.\n","\n","Great, we are done with preprocessing our data, let’s build our model\n","now."],"metadata":{"id":"n6hL3-JvpJpM"}},{"cell_type":"markdown","source":["#### Model Design\n","\n","<!-- #endregion -->\n","<!-- #region id=\"wkEEsz7jzIJG\" -->\n","\n","We’ll create a simple model for text classification. The model is\n","composed of an `nn.Embedding` plus a linear layer for the classication\n","purpose. <!-- #endregion -->\n","\n","<!-- #region id=\"Al-KQSHw1zpg\" -->\n","\n","<strong>But, what is the embedding layer, why did we use it, and how\n","does it work?</strong>\n","\n","We could feed the list of tokenized sequences e,g.\n","`[8, 124, 3732,  ..., 1197, 71, 4635],[145, 2402, 1,  ..., 74, 57, 4],         ...,`\n","directly to our model classifier layer, but our model won’t make sense\n","of the meaning of the words in our sentences or the relationship between\n","the words in the sequence. The technique that we will be using here to\n","do so is called the word embeddings with the help of `Embedding` layer\n","from `torch.nn`.\n","\n","Word embedding is a technique where individual words are represented as\n","real-valued vectors in a lower-dimensional space and captures inter-word\n","semantics. Word embeddings can be used to store word meanings and\n","retrieve them using indices, as well as to measure the similarity or\n","distance between words based on their vectors.\n","\n","The benefit of word embeddings is that they can preserve syntactic and\n","semantic information of words and reduce the dimensionality of text data\n","compared to other methods such as one-hot encoding or bag-of-words.\n","\n","To learn more about the advantages of word embeddings compared to other\n","methods, see [this\n","link](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp).\n","\n","One way to best learn the embeddings of the words is to use it along\n","with our text classification task. The `nn.Embedding` layer makes this\n","possible, and the backpropagation technique of model trainining will\n","learn the best weights of the layer along with our task.\n","\n","The output of the nn.embedding layer in the example is a tensor of shape\n","(sequence_length, batch_size, embedding_dim) that contains the\n","embeddings for each token in the input text. It’s common to see word\n","embeddings that are 256-dimensional, 512-dimensional, or\n","1024-dimensional when dealing with very large vocabularies. In our case\n","we’ll set the output dimension as 100.\n","\n","Hence, our model defintion will be as follows:"],"metadata":{"id":"Chp44oHxpJpM"}},{"cell_type":"code","source":["from torch import nn\n","import torch.nn.functional as F\n","\n","class TextClassificationModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, output_dim):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","\n","    def forward(self, text):\n","        embedded = self.embedding(text)\n","        embedded = embedded.permute(1, 0, 2)\n","        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)\n","        return self.fc(pooled)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:18:46.645412Z","iopub.execute_input":"2023-05-21T07:18:46.646317Z","iopub.status.idle":"2023-05-21T07:18:46.656672Z","shell.execute_reply.started":"2023-05-21T07:18:46.646253Z","shell.execute_reply":"2023-05-21T07:18:46.654782Z"},"trusted":true,"id":"3tpCcxNQpJpM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our model is a simple text classification model that consists of two\n","layers: an embedding layer and a linear layer. It first applies the\n","embedding layer to get a low-dimensional representation of each token in\n","the text. Then it permutes the dimensions of the embedded tensor to\n","match the expected input shape of the average pooling layer. Next, it\n","applies the average pooling layer to get a fixed-length representation\n","of the whole text by averaging over all tokens. Finally, it passes this\n","representation to the linear layer to get a single output value for each\n","text.\n","\n","The structure and layers of the model are shown below:\n","\n","Input text -\\> Embedding layer -\\> Average pooling -\\> Linear layer -\\>\n","Output value"],"metadata":{"id":"3_6dfzD6pJpN"}},{"cell_type":"code","source":["#  Cree una instancia del modelo de clasificación de texto con el tamaño de vocabulario especificado, la dimensión de incrustación y la dimensión de salida\n","\n","model = TextClassificationModel(vocab_size=len(vocab), embedding_dim=100, output_dim=4)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:19:37.109379Z","iopub.execute_input":"2023-05-21T07:19:37.109755Z","iopub.status.idle":"2023-05-21T07:19:37.203712Z","shell.execute_reply.started":"2023-05-21T07:19:37.109724Z","shell.execute_reply":"2023-05-21T07:19:37.202761Z"},"trusted":true,"id":"pF6YJesTpJpN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a loss function based on binary cross entropy and sigmoid activation\n","criterion = nn.CrossEntropyLoss()\n","# Define an optimizer that updates the model parameters using Adam algorithm\n","model = TextClassificationModel(vocab_size=len(vocab), embedding_dim=100, output_dim=4)\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","# Move the model to the device (CPU or GPU) for computation\n","model = model.to(device)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:20:03.533931Z","iopub.execute_input":"2023-05-21T07:20:03.534484Z","iopub.status.idle":"2023-05-21T07:20:03.553687Z","shell.execute_reply.started":"2023-05-21T07:20:03.534445Z","shell.execute_reply":"2023-05-21T07:20:03.552633Z"},"trusted":true,"id":"BUIiEYqepJpN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# En el bucle de entrenamiento\n","for epoch in range(20):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","    for label, text in train_loader:\n","        optimizer.zero_grad()\n","        predictions = model(text).squeeze(1)\n","        loss = criterion(predictions, label - 1)  # Ajusta las etiquetas en la pérdida\n","\n","        _, predicted = torch.max(predictions, 1)\n","        correct = (predicted + 1 == label).float()  # Ajusta la comparación con las etiquetas\n","\n","        acc = correct.sum() / len(correct)\n","\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","    print(\"Epoch %d Train: Loss: %.4f Acc: %.4f\" % (epoch + 1, epoch_loss / len(train_loader),\n","                                                    epoch_acc / len(train_loader)))\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for label, text in val_loader:\n","            predictions = model(text).squeeze(1)\n","            loss = criterion(predictions, label - 1)  # Ajusta las etiquetas en la pérdida\n","\n","            # Corrige el cálculo de la precisión\n","            _, predicted = torch.max(predictions, 1)\n","            correct = (predicted + 1 == label).float()  # Ajusta la comparación con las etiquetas\n","\n","            acc = correct.sum() / len(correct)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","    print(\"Epoch %d Valid: Loss: %.4f Acc: %.4f\" % (epoch + 1, epoch_loss / len(val_loader),\n","                                                    epoch_acc / len(val_loader)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wp8vp9NgWSaC","executionInfo":{"status":"ok","timestamp":1700670621377,"user_tz":240,"elapsed":698046,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"4d06f0c1-d4d5-473b-e274-8346fb300142"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Train: Loss: 1.3822 Acc: 0.2813\n","Epoch 1 Valid: Loss: 1.3587 Acc: 0.3592\n","Epoch 2 Train: Loss: 1.3512 Acc: 0.3872\n","Epoch 2 Valid: Loss: 1.3267 Acc: 0.4443\n","Epoch 3 Train: Loss: 1.3135 Acc: 0.4839\n","Epoch 3 Valid: Loss: 1.2896 Acc: 0.5384\n","Epoch 4 Train: Loss: 1.2721 Acc: 0.5381\n","Epoch 4 Valid: Loss: 1.2446 Acc: 0.4798\n","Epoch 5 Train: Loss: 1.2140 Acc: 0.5946\n","Epoch 5 Valid: Loss: 1.1856 Acc: 0.6035\n","Epoch 6 Train: Loss: 1.1545 Acc: 0.6603\n","Epoch 6 Valid: Loss: 1.1232 Acc: 0.6856\n","Epoch 7 Train: Loss: 1.0913 Acc: 0.6802\n","Epoch 7 Valid: Loss: 1.0672 Acc: 0.6831\n","Epoch 8 Train: Loss: 1.0240 Acc: 0.7066\n","Epoch 8 Valid: Loss: 1.0062 Acc: 0.7153\n","Epoch 9 Train: Loss: 0.9662 Acc: 0.7358\n","Epoch 9 Valid: Loss: 0.9600 Acc: 0.6876\n","Epoch 10 Train: Loss: 0.9077 Acc: 0.7723\n","Epoch 10 Valid: Loss: 0.9092 Acc: 0.7367\n","Epoch 11 Train: Loss: 0.8569 Acc: 0.7797\n","Epoch 11 Valid: Loss: 0.8707 Acc: 0.7099\n","Epoch 12 Train: Loss: 0.8106 Acc: 0.7979\n","Epoch 12 Valid: Loss: 0.8292 Acc: 0.7352\n","Epoch 13 Train: Loss: 0.7671 Acc: 0.8042\n","Epoch 13 Valid: Loss: 0.7969 Acc: 0.7427\n","Epoch 14 Train: Loss: 0.7291 Acc: 0.8173\n","Epoch 14 Valid: Loss: 0.7616 Acc: 0.7400\n","Epoch 15 Train: Loss: 0.6904 Acc: 0.8307\n","Epoch 15 Valid: Loss: 0.7359 Acc: 0.7715\n","Epoch 16 Train: Loss: 0.6534 Acc: 0.8368\n","Epoch 16 Valid: Loss: 0.7096 Acc: 0.7605\n","Epoch 17 Train: Loss: 0.6212 Acc: 0.8391\n","Epoch 17 Valid: Loss: 0.6944 Acc: 0.7715\n","Epoch 18 Train: Loss: 0.5982 Acc: 0.8450\n","Epoch 18 Valid: Loss: 0.6641 Acc: 0.7831\n","Epoch 19 Train: Loss: 0.5732 Acc: 0.8482\n","Epoch 19 Valid: Loss: 0.6450 Acc: 0.7834\n","Epoch 20 Train: Loss: 0.5445 Acc: 0.8558\n","Epoch 20 Valid: Loss: 0.6242 Acc: 0.8027\n"]}]},{"cell_type":"markdown","source":["Train the model for 10 epochs, print the training and validation loss\n","and accuracy for each epoch:"],"metadata":{"id":"M48oDax-pJpN"}},{"cell_type":"markdown","source":["After 10 epochs of training, we get around 89% accuracy on the\n","validation dataset (note, your result may vary). This is still a simple\n","model, the code can be modified or extended by changing some parameters\n","or adding more layers.\n","\n","Let’s see how it performs on the test dataset, but let’s first prepare\n","it as we did for the training and validation datasets before."],"metadata":{"id":"CcrhwfBxpJpN"}},{"cell_type":"code","source":["# Read the text data and labels from the test directory\n","test_labels, test_texts = read_text_data(data_path/'test')\n","\n","# Create a custom text dataset object for the test data using the vocabulary and numericalize function\n","test_dataset = CustomTextDataset(test_labels, test_texts, vocab, numericalize_text)\n","\n","# Create a data loader for the test dataset\n","test_loader = DataLoader(test_dataset, collate_fn=collate_batch, batch_size=batch_size, shuffle=False)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:24:47.450899Z","iopub.execute_input":"2023-05-21T07:24:47.451396Z","iopub.status.idle":"2023-05-21T07:24:48.539180Z","shell.execute_reply.started":"2023-05-21T07:24:47.451360Z","shell.execute_reply":"2023-05-21T07:24:48.537888Z"},"trusted":true,"id":"YJRXKv0_pJpY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loss = 0\n","test_acc = 0\n","model.eval()\n","\n","with torch.no_grad():\n","    for label, text in test_loader:\n","        predictions = model(text).squeeze(1)\n","        loss = criterion(predictions, label - 1)  # Ajusta las etiquetas en la pérdida\n","\n","        # Ajusta la comparación de las etiquetas en la evaluación\n","        # Corrige el cálculo de la precisión\n","        _, predicted = torch.max(predictions, 1)\n","        correct = (predicted + 1 == label).float()  # Ajusta la comparación con las etiquetas\n","\n","        acc = correct.sum() / len(correct)\n","\n","        test_loss += loss.item()\n","        test_acc += acc.item()\n","\n","print(\"Test: Loss: %.4f Acc: %.4f\" %\n","      (test_loss / len(test_loader),\n","       test_acc / len(test_loader)))"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:25:24.564470Z","iopub.execute_input":"2023-05-21T07:25:24.564941Z","iopub.status.idle":"2023-05-21T07:25:37.040354Z","shell.execute_reply.started":"2023-05-21T07:25:24.564906Z","shell.execute_reply":"2023-05-21T07:25:37.039363Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"UNYQsFZ0pJpZ","executionInfo":{"status":"ok","timestamp":1700671377398,"user_tz":240,"elapsed":15116,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"34e878d0-37c3-47e3-c787-1e504d625fa9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test: Loss: 1.0124 Acc: 0.6600\n"]}]},{"cell_type":"code","source":["label_map = {'sis': 0, 'dis': 1, 'cic': 2, 'tis': 3}"],"metadata":{"id":"4DNvPuAyV2Xb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Tokeniza la frase\n","#tokenized_text = numericalize_text(\"Me gusta resolver ejercicios de seguridad.\")\n","tokenized_text = numericalize_text(\"Me apasiona la creatividad y quiero canalizar mi expresión artística a través del diseño y la animación digital.\")\n","# tokenized_text = numericalize_text(\"Estoy ansioso por aprender sobre ciberseguridad y contribuir a la protección de la información digital.\")\n","# tokenized_text = numericalize_text(\"Quiero ser parte del diseño de sistemas de detección de intrusiones que protejan activamente las redes.\")\n","# Convierte la lista de tokens a un tensor y agrega una dimensión adicional\n","input_text = torch.tensor(tokenized_text).unsqueeze(1).to(device)\n","\n","# Pasa la entrada a través del modelo\n","with torch.no_grad():\n","    model.eval()\n","    output = model(input_text)\n","\n","# Obtiene las predicciones\n","_, predicted_label = torch.max(output, 1)\n","\n","# Imprime la etiqueta predicha\n","print(\"Etiqueta predicha:\", predicted_label.item()+1)\n","predicted_category = list(label_map.keys())[predicted_label.item()]\n","\n","# Imprime la categoría predicha\n","print(\"Categoría predicha:\", predicted_category)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCF1QgQrikJR","executionInfo":{"status":"ok","timestamp":1700670637871,"user_tz":240,"elapsed":55,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"1ed26997-8948-4ae7-ca76-40917e904d85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Etiqueta predicha: 2\n","Categoría predicha: dis\n"]}]},{"cell_type":"markdown","source":["We get around 88% accuracy on the test dataset. Not bad!\n","\n","Let’s see how it works on some random input text data, we’ll do some\n","housekeeping jobs first: <!-- #endregion -->"],"metadata":{"id":"Dt6lNKKzpJpZ"}},{"cell_type":"markdown","source":["Well done! Our model gives a very low score for a negative sentiment and\n","gives 1 for a positive movie review.\n","\n","Our model is doing great, let’s save it for inference later on:"],"metadata":{"id":"tzg7_PhApJpa"}},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/IA22023/DATASETS/ticsclassification-model.pt')"],"metadata":{"execution":{"iopub.status.busy":"2023-05-21T07:29:35.758114Z","iopub.execute_input":"2023-05-21T07:29:35.758473Z","iopub.status.idle":"2023-05-21T07:29:35.870056Z","shell.execute_reply.started":"2023-05-21T07:29:35.758447Z","shell.execute_reply":"2023-05-21T07:29:35.868764Z"},"trusted":true,"id":"-3s84HTwpJpa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model = TextClassificationModel(vocab_size=len(vocab), embedding_dim=100, output_dim=4) # Crea una nueva instancia del modelo\n","\n","# Cargamos los pesos del modelo guardado\n","loaded_model.load_state_dict(torch.load('/content/drive/MyDrive/IA22023/DATASETS/ticsclassification-model.pt'))\n","loaded_model = loaded_model.to(device)\n","loaded_model.eval()\n","\n","\n","new_tokenized_text = numericalize_text(\"Me gusta resolver ejercicios de lógica.\") # Tokeniza la nueva frase\n","\n","# Convierte la lista de tokens a un tensor y agrega una dimensión adicional\n","new_input_text = torch.tensor(new_tokenized_text).unsqueeze(1).to(device)\n","\n","# Pasa la entrada a través del modelo cargado\n","with torch.no_grad():\n","    output = loaded_model(new_input_text)\n","\n","# Obtiene las predicciones\n","_, predicted_label = torch.max(output, 1)\n","\n","# Imprime la etiqueta predicha\n","print(\"Etiqueta predicha:\", predicted_label.item() + 1)\n","\n","# Mapea las etiquetas predichas a las categorías correspondientes\n","predicted_category = list(label_map.keys())[predicted_label.item()]\n","\n","# Imprime la categoría predicha\n","print(\"Categoría predicha:\", predicted_category)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"im89_EpEnAr2","executionInfo":{"status":"ok","timestamp":1700679269160,"user_tz":240,"elapsed":366,"user":{"displayName":"Reynaldo Kama Copa","userId":"16264202174887145069"}},"outputId":"a622940a-6d61-43a9-d0ac-131e07fb2e3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Etiqueta predicha: 3\n","Categoría predicha: cic\n"]}]}]}